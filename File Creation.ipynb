{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2f900b-f92e-4cb3-b1d9-2969dd8f2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, urllib.parse, csv # For storing data and manipulating it\n",
    "\n",
    "import requests # For API requests\n",
    "import pandas as pd # For data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261a94bf-2845-45dd-babe-5d37ca585e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n",
    "# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<ecorpron@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This is the file that contains all of the dinasaur article headers\n",
    "df = pd.read_csv(\"data/dinosaur_genera.cleaned.SEPT.2022 - dinosaur_genera.cleaned.SEPT.2022.csv\")\n",
    "ARTICLE_TITLES = df.name\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. In the example, below, we only vary the article name, so the majority of the fields\n",
    "# can stay constant for each request. Of course, these values *could* be changed if necessary.\n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"desktop\",\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",\n",
    "    \"end\":         \"2022093000\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365b804a-2428-4a23-80ec-00a36682c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "# Performs a single API request for an article. Will return a json response broken into months of the article\n",
    "# in the specified date ranges\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(article_title.replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b83fbc2b-133a-421c-a003-d89d92f4f59e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "ending\n"
     ]
    }
   ],
   "source": [
    "print('Starting API requests for desktop views for desktop dinosaur articles')\n",
    "\n",
    "# Creates a new file with specified name, even if it already exists\n",
    "with open('dino_monthly_desktop_start201501-end202209.json', 'w') as outfile:    \n",
    "    fileToBe = []\n",
    "    \n",
    "    # Loops through each article.\n",
    "    # Skips the first one since it appears to be a dead link\n",
    "    for article in ARTICLE_TITLES[1:]:\n",
    "        # retrieves the view information\n",
    "        views = request_pageviews_per_article(article)\n",
    "        \n",
    "        # Loops through each month of data in the view\n",
    "        for month in views['items']:\n",
    "            # Copies the month over to be saved\n",
    "            fileToBe.append(month)\n",
    "    # Saves the data as a json file\n",
    "    json.dump(fileToBe, outfile, indent = 4)\n",
    "print('API requests finished, file created with name '+'dino_monthly_desktop_start201501-end202209.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe9f3116-2599-4a52-9619-1a645af395e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "ending\n"
     ]
    }
   ],
   "source": [
    "# Sets values for mobile-app view data\n",
    "articlePageviewsParamsMobileApp = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"mobile-app\",\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",\n",
    "    \"end\":         \"2022093000\"\n",
    "}\n",
    "# Sets values for mobile-web view data\n",
    "articlePageviewsParamsMobileWeb = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"mobile-web\",\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",\n",
    "    \"end\":         \"2022093000\"\n",
    "}\n",
    "\n",
    "print('Starting API requests for desktop views for all web dinosaur articles')\n",
    "with open('dino_monthly_mobile_start201501-end202209.json', 'w') as outfile:\n",
    "    fileToBe = []\n",
    "\n",
    "    # Runs through all articles\n",
    "    for article in ARTICLE_TITLES[1:]:\n",
    "        # Gets the views for both web access ways\n",
    "        appViews = request_pageviews_per_article(article, request_template = articlePageviewsParamsMobileApp)\n",
    "        webViews = request_pageviews_per_article(article, request_template = articlePageviewsParamsMobileWeb)\n",
    "        \n",
    "        for i in range(len(appViews['items'])):\n",
    "            # Goes through each month and adds the view data together\n",
    "            appViews['items'][i]['views'] += webViews['items'][i]['views']\n",
    "            fileToBe.append(appViews['items'][i])\n",
    "    # Saves the combined view data in a json file\n",
    "    json.dump(fileToBe, outfile, indent = 4)\n",
    "print('API requests finished, file created with name '+'dino_monthly_mobile_start201501-end202209.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410b6dfc-e98c-4e15-baf0-4d85e74d222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "ending\n"
     ]
    }
   ],
   "source": [
    "#  Sets values for cumulative views\n",
    "articlePageviewsParamsAll = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"all-access\",\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       \"2015010100\",\n",
    "    \"end\":         \"2022093000\"\n",
    "}\n",
    "\n",
    "print('Starting API requests for all views on dinosaur articles')\n",
    "with open('dino_monthly_cumulative_start201501-end202209.json', 'w') as outfile:\n",
    "    fileToBe = []\n",
    "    \n",
    "    # Runs through all articles\n",
    "    for article in ARTICLE_TITLES[1:]:\n",
    "        # Gets the views for current article\n",
    "        views = request_pageviews_per_article(article, request_template = articlePageviewsParamsAll)\n",
    "        \n",
    "        # resets total to zero\n",
    "        total = 0\n",
    "        \n",
    "        # Runs through each month of data\n",
    "        for month in views['items']:\n",
    "            # sums all previous months with current month\n",
    "            total += month['views']\n",
    "            # saves into current month viewing\n",
    "            month['views'] = total\n",
    "            fileToBe.append(month)\n",
    "    \n",
    "    # Saves cumulative view data in JSON file\n",
    "    json.dump(fileToBe, outfile, indent = 4)\n",
    "print('API requests finished, file created with name '+'dino_monthly_cumulative_start201501-end202209.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
