{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694d231b-2627-40db-9249-92a1caa1a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, urllib.parse, csv # For storing data and manipulating it\n",
    "\n",
    "import requests # For API requests\n",
    "import pandas as pd # For data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ab69bf-4afe-45db-b66e-a9ff779936a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "dfPol = pd.read_csv(\"data/politicians_by_country_SEPT.2022.csv\")\n",
    "dfPop = pd.read_csv(\"data/population_by_country_2022.csv\")\n",
    "#print(dfPol.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3945bf-bfb8-4943-a9e1-dacb040da4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Geography  Population (millions)           Region\n",
      "0   Algeria                   44.9  NORTHERN AFRICA\n",
      "1     Egypt                  103.5  NORTHERN AFRICA\n",
      "2     Libya                    6.8  NORTHERN AFRICA\n",
      "3   Morocco                   36.7  NORTHERN AFRICA\n",
      "4     Sudan                   46.9  NORTHERN AFRICA\n"
     ]
    }
   ],
   "source": [
    "# Convert dfPop to a dict ordered file\n",
    "dfPop[\"Region\"] = \"\"\n",
    "#print(dfPop.loc[0,\"Geography\"])\n",
    "reg = \"\"\n",
    "for i in range(len(dfPop.index)):\n",
    "    geo = dfPop.loc[i, \"Geography\"]\n",
    "    if(geo.isupper()):\n",
    "        reg = geo\n",
    "    dfPop.loc[i, \"Region\"] = reg\n",
    "    if(reg == geo):\n",
    "        dfPop = dfPop.drop([i])\n",
    "dfPop = dfPop.reset_index(drop = True)\n",
    "print(dfPop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa38d20-2552-4263-988e-47b6cae54dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The basic English Wikipedia API endpoint\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# We'll assume that there needs to be some throttling for these requests - we should always be nice to a free data resource\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'ecorpron, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This is just a list of English Wikipedia article titles that we can use for example requests\n",
    "ARTICLE_TITLES = dfPol.name\n",
    "\n",
    "# This is a string of additional page properties that can be returned see the Info documentation for\n",
    "# what can be included. If you don't want any this can simply be the empty string\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "# This template lists the basic parameters for making this\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "\n",
    "# The current ORES API endpoint\n",
    "API_ORES_SCORE_ENDPOINT = \"https://ores.wikimedia.org/v3\"\n",
    "# A template for mapping to the URL\n",
    "API_ORES_SCORE_PARAMS = \"/scores/{context}/{revid}/{model}\"\n",
    "\n",
    "# Use some delays so that we do not hammer the API with our requests\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making automated requests we should include something that is unique to the person making the request\n",
    "# This should include an email - your UW email would be good to put in there\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<uwnetid@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022'\n",
    "}\n",
    "\n",
    "# This template lists the basic parameters for making an ORES request\n",
    "ORES_PARAMS_TEMPLATE = {\n",
    "    \"context\": \"enwiki\",        # which WMF project for the specified revid\n",
    "    \"revid\" : \"\",               # the revision to be scored - this will probably change each call\n",
    "    \"model\": \"articlequality\"   # the AI/ML scoring model to apply to the reviewion\n",
    "}\n",
    "#\n",
    "# The current ML models for English wikipedia are:\n",
    "#   \"articlequality\"\n",
    "#   \"articletopic\"\n",
    "#   \"damaging\"\n",
    "#   \"version\"\n",
    "#   \"draftquality\"\n",
    "#   \"drafttopic\"\n",
    "#   \"goodfaith\"\n",
    "#   \"wp10\"\n",
    "#\n",
    "# The specific documentation on these is scattered so if you want to use one you'll have to look around.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235f2b5-cb6c-4f3f-8e05-42cecbc54bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    request_template['titles'] = article_title\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or any other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, \n",
    "                                   endpoint_url = API_ORES_SCORE_ENDPOINT, \n",
    "                                   endpoint_params = API_ORES_SCORE_PARAMS, \n",
    "                                   request_template = ORES_PARAMS_TEMPLATE,\n",
    "                                   headers = REQUEST_HEADERS,\n",
    "                                   features=False):\n",
    "    # Make sure we have an article revision id\n",
    "    if not article_revid: return None\n",
    "    \n",
    "    # set the revision id into the template\n",
    "    request_template['revid'] = article_revid\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # the features used by the ML model can sometimes be returned as well as scores\n",
    "    if features:\n",
    "        request_url = request_url+\"?features=true\"\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like ORES - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584ea5a1-39e9-4497-9131-bf4179ab69a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Article Access\n",
      "Finished Article Access\n",
      "           article_title revision_id article_quality\n",
      "0        Shahjahan Noori  1099689043              GA\n",
      "1  Abdul Ghafar Lakanwal   943562276           Start\n",
      "2         Majah Ha Adrif   852404094           Start\n",
      "3      Haroon al-Afghani  1095102390               B\n",
      "4            Tayyab Agha  1104998382           Start\n"
     ]
    }
   ],
   "source": [
    "dfArticles = pd.DataFrame(columns = [\"article_title\", \"revision_id\", \"article_quality\"])\n",
    "articlesWithNoScore = []\n",
    "\n",
    "print(\"Starting Article Access\")\n",
    "for i in range(len(ARTICLE_TITLES)):\n",
    "    dfArticles.loc[i, \"article_title\"] = ARTICLE_TITLES[i]\n",
    "    info = request_pageinfo_per_article(ARTICLE_TITLES[i])\n",
    "\n",
    "    articleKey = list(info[\"query\"][\"pages\"].keys())[0]\n",
    "    revId = info[\"query\"][\"pages\"][articleKey].get(\"lastrevid\")\n",
    "    dfArticles.loc[i, \"revision_id\"] = revId\n",
    "    \n",
    "    score = request_ores_score_per_article(revId)\n",
    "    \n",
    "    if score == None:\n",
    "        dfArticles.loc[i, \"article_quality\"] = None\n",
    "    else:\n",
    "        scoreKey = list(score[\"enwiki\"][\"scores\"].keys())[0]\n",
    "        rating = score['enwiki'][\"scores\"][scoreKey][\"articlequality\"][\"score\"].get(\"prediction\")\n",
    "        dfArticles.loc[i, \"article_quality\"] = rating\n",
    "print(\"Finished Article Access\")\n",
    "print(dfArticles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa56b18-119c-41a5-bfc1-be8acbb4855c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           article_title revision_id article_quality      Country\n",
      "0        Shahjahan Noori  1099689043              GA  Afghanistan\n",
      "1  Abdul Ghafar Lakanwal   943562276           Start  Afghanistan\n",
      "2         Majah Ha Adrif   852404094           Start  Afghanistan\n",
      "3      Haroon al-Afghani  1095102390               B  Afghanistan\n",
      "4            Tayyab Agha  1104998382           Start  Afghanistan\n"
     ]
    }
   ],
   "source": [
    "dfArticles[\"Country\"] = \"\"\n",
    "for i in range(len(dfArticles)):\n",
    "    dfArticles.loc[i, \"Country\"] = dfPol.loc[i, \"country\"]\n",
    "print(dfArticles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0007414a-d164-4bc2-a0d2-d9b2a301efeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Western Sahara', 'Mauritius', 'Mayotte', 'Reunion', 'Sao Tome and Principe', 'eSwatini', 'Canada', 'United States', 'Curacao', 'Guadeloupe']\n",
      "           article_title revision_id article_quality      Country      Region  \\\n",
      "0        Shahjahan Noori  1099689043              GA  Afghanistan  SOUTH ASIA   \n",
      "1  Abdul Ghafar Lakanwal   943562276           Start  Afghanistan  SOUTH ASIA   \n",
      "2         Majah Ha Adrif   852404094           Start  Afghanistan  SOUTH ASIA   \n",
      "3      Haroon al-Afghani  1095102390               B  Afghanistan  SOUTH ASIA   \n",
      "4            Tayyab Agha  1104998382           Start  Afghanistan  SOUTH ASIA   \n",
      "\n",
      "   Population  \n",
      "0        41.1  \n",
      "1        41.1  \n",
      "2        41.1  \n",
      "3        41.1  \n",
      "4        41.1  \n"
     ]
    }
   ],
   "source": [
    "no_match = []\n",
    "dfArticles[\"Region\"] = \"\"\n",
    "dfArticles[\"Population\"] = 0.0\n",
    "for i in range(len(dfPop)):\n",
    "    matched = False\n",
    "    country = dfPop.loc[i, \"Geography\"]\n",
    "    for j in range(len(dfArticles)):\n",
    "        if country == dfArticles.loc[j, \"Country\"]:\n",
    "            matched = True\n",
    "            dfArticles.loc[j, \"Region\"] = dfPop.loc[i, \"Region\"]\n",
    "            dfArticles.loc[j, \"Population\"] = dfPop.loc[i, \"Population (millions)\"]\n",
    "    if matched == False:\n",
    "        no_match.append(country)\n",
    "print(no_match[0:10])\n",
    "print(dfArticles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5fee10-135a-492e-8435-c906cef1103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'wp_countries-no_match.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(no_match))\n",
    "dfArticles.dropna()\n",
    "dfArticles.to_csv('wp_politicians_by_country.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780462a-cb7d-439d-bb20-eb900a1ccf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
